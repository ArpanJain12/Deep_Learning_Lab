# -*- coding: utf-8 -*-
"""Experiment-02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kh_gn2VWx3H697T9vIB7eN5J2A943IkT
"""

import torch
import torchvision
import numpy as np
import matplotlib.pyplot as plt

"""Load MNIST & Convert to NumPy

"""

train_dataset = torchvision.datasets.MNIST(
    root='./data',
    train=True,
    transform=torchvision.transforms.ToTensor(),
    download=True
)

val_dataset = torchvision.datasets.MNIST(
    root='./data',
    train=False,
    transform=torchvision.transforms.ToTensor(),
    download=True
)

train_loader = torch.utils.data.DataLoader(
    dataset=train_dataset,
    batch_size=64,
    shuffle=True
)

val_loader = torch.utils.data.DataLoader(
    dataset=val_dataset,
    batch_size=64,
    shuffle=False
)

"""One-Hot Encoding"""

def one_hot(labels, num_classes=10):
    return np.eye(num_classes)[labels]

"""Activation Functions"""

def relu(z):
    return np.maximum(0, z)

def relu_derivative(z):
    return (z > 0).astype(float)

def sigmoid(z):
    return 1 / (1 + np.exp(-z))

def sigmoid_derivative(z):
    s = sigmoid(z)
    return s * (1 - s)

def tanh(z):
    return np.tanh(z)

def tanh_derivative(z):
    return 1 - np.tanh(z)**2

"""Softmax (Output Layer)"""

def softmax(z):
    z = z - np.max(z, axis=1, keepdims=True)
    exp_z = np.exp(z)
    return exp_z / np.sum(exp_z, axis=1, keepdims=True)

"""Neural Network Class"""

class NeuralNetwork:
    def __init__(self, layer_sizes, activation='relu', lr=0.01):
        self.layer_sizes = layer_sizes
        self.lr = lr
        self.weights = {}
        self.biases = {}
        self.z = {}
        self.a = {}

        if activation == 'relu':
            self.activation = relu
            self.activation_derivative = relu_derivative
        elif activation == 'sigmoid':
            self.activation = sigmoid
            self.activation_derivative = sigmoid_derivative
        elif activation == 'tanh':
            self.activation = tanh
            self.activation_derivative = tanh_derivative
        else:
            raise ValueError("Unsupported activation")

        # Initialize parameters
        for i in range(len(layer_sizes) - 1):
            self.weights[i] = np.random.randn(
                layer_sizes[i], layer_sizes[i + 1]
            ) * 0.01
            self.biases[i] = np.zeros((1, layer_sizes[i + 1]))


    # Forward Propagation
    def forward(self, X):
        self.a[0] = X

        for i in range(len(self.layer_sizes) - 2):
            self.z[i + 1] = np.dot(self.a[i], self.weights[i]) + self.biases[i]
            self.a[i + 1] = self.activation(self.z[i + 1])

        # Output layer
        z_out = np.dot(self.a[len(self.layer_sizes) - 2],
                       self.weights[len(self.layer_sizes) - 2]) + \
                self.biases[len(self.layer_sizes) - 2]

        self.a[len(self.layer_sizes) - 1] = softmax(z_out)
        return self.a[len(self.layer_sizes) - 1]

    def compute_loss(self, y_true, y_pred):
        eps = 1e-8
        return -np.mean(np.sum(y_true * np.log(y_pred + eps), axis=1))

    # Backward Propagation
    def backward(self, y_true):
        grads_w = {}
        grads_b = {}

        L = len(self.layer_sizes) - 1
        delta = self.a[L] - y_true

        for i in reversed(range(L)):
            grads_w[i] = np.dot(self.a[i].T, delta) / y_true.shape[0]
            grads_b[i] = np.mean(delta, axis=0, keepdims=True)

            if i > 0:
                delta = np.dot(delta, self.weights[i].T) * \
                        self.activation_derivative(self.z[i])

        return grads_w, grads_b

    # Update Parameters
    def update_parameters(self, grads_w, grads_b):
        for i in self.weights:
            self.weights[i] -= self.lr * grads_w[i]
            self.biases[i] -= self.lr * grads_b[i]

    # Prediction & Evaluation
    def predict(self, X):
        probs = self.forward(X)
        return np.argmax(probs, axis=1)

    def evaluate(self, X, y):
        preds = self.predict(X)
        return np.mean(preds == y)

"""Model Training"""

def train_model(model, train_loader, val_loader, epochs=10):
    history = {
        'train_loss': [],
        'train_acc': [],
        'val_loss': [],
        'val_acc': []
    }

    for epoch in range(epochs):
        train_losses = []
        train_accs = []

        for images, labels in train_loader:
            images = images.cpu().numpy()
            labels = labels.cpu().numpy()

            X = images.reshape(images.shape[0], -1)
            y = one_hot(labels)

            preds = model.forward(X)
            loss = model.compute_loss(y, preds)

            grads_w, grads_b = model.backward(y)
            model.update_parameters(grads_w, grads_b)

            train_losses.append(loss)
            train_accs.append(model.evaluate(X, labels))

        # Validation
        val_losses = []
        val_accs = []

        for images, labels in val_loader:
            images = images.cpu().numpy()
            labels = labels.cpu().numpy()

            X = images.reshape(images.shape[0], -1)
            y = one_hot(labels)

            preds = model.forward(X)
            val_losses.append(model.compute_loss(y, preds))
            val_accs.append(model.evaluate(X, labels))

        history['train_loss'].append(np.mean(train_losses))
        history['train_acc'].append(np.mean(train_accs))
        history['val_loss'].append(np.mean(val_losses))
        history['val_acc'].append(np.mean(val_accs))

        print(f"Epoch {epoch+1}/{epochs} | "
              f"Train Acc: {history['train_acc'][-1]:.4f} | "
              f"Val Acc: {history['val_acc'][-1]:.4f}")

    return history

experiments = [
    {
        "name": "relu_1",
        "layers": [784, 128, 10],
        "activation": "relu",
        "lr": 0.1
    },
    {
        "name": "relu_2",
        "layers": [784, 128, 64, 10],
        "activation": "relu",
        "lr": 0.1
    },
    {
        "name": "sigmoid",
        "layers": [784, 128, 64, 10],
        "activation": "sigmoid",
        "lr": 0.1
    },
    {
        "name": "tanh",
        "layers": [784, 128, 64, 10],
        "activation": "tanh",
        "lr": 0.1
    }
]

"""Save Plots

"""

import csv
import os

os.makedirs("results", exist_ok=True)
os.makedirs("plots", exist_ok=True)

results = []

for exp in experiments:
    print(f"\nRunning Experiment: {exp['name']}")

    model = NeuralNetwork(
        layer_sizes=exp["layers"],
        activation=exp["activation"],
        lr=exp["lr"]
    )

    history = train_model(
        model,
        train_loader,
        val_loader,
        epochs=10
    )

    # Save final metrics
    results.append({
        "Experiment": exp["name"],
        "Layers": exp["layers"],
        "Activation": exp["activation"],
        "Learning Rate": exp["lr"],
        "Final Train Acc": history["train_acc"][-1],
        "Final Val Acc": history["val_acc"][-1],
        "Final Train Loss": history["train_loss"][-1],
        "Final Val Loss": history["val_loss"][-1]
    })

    # Plot accuracy
    plt.figure()
    plt.plot(history["train_acc"], label="Train Accuracy")
    plt.plot(history["val_acc"], label="Validation Accuracy")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.title(exp["name"])
    plt.legend()
    plt.savefig(f"plots/{exp['name']}_accuracy.png")
    plt.close()

    # Plot loss
    plt.figure()
    plt.plot(history["train_loss"], label="Train Loss")
    plt.plot(history["val_loss"], label="Validation Loss")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title(exp["name"])
    plt.legend()
    plt.savefig(f"plots/{exp['name']}_loss.png")
    plt.close()

import os
from IPython.display import Image, display

for file in os.listdir("plots"):
    if "accuracy" in file:
        display(Image(filename=f"plots/{file}"))